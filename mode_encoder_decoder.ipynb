{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ebde34",
   "metadata": {},
   "source": [
    "**Model Config**\n",
    "Use Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03d65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import data_module as dm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SOS = 0\n",
    "EOS = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58638407",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "- Load CSV file dataset\n",
    "- Create Torch Dataset\n",
    "- Create Torch DataLoader\n",
    "- Create padding func (with insertion of SOS and EOS tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d4005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl, train_len, test_len = dm.get_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "693bd733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.714544553675216\n",
      "0.7147042869721002\n",
      "0.7152992678686013\n",
      "0.7157691516144529\n",
      "0.7158640362264587\n",
      "0.7184338336969268\n",
      "0.7184332325845597\n",
      "0.7184166664027718\n",
      "0.7184190267107762\n",
      "0.7184198134816941\n",
      "0.7224914225019586\n",
      "0.7221516882201611\n",
      "0.7224492155664712\n",
      "0.7228669847956202\n",
      "0.7228608816884274\n",
      "0.7231692316838167\n",
      "0.7227879287091186\n",
      "0.722410758929971\n",
      "0.722520916725233\n",
      "0.7225518670181337\n",
      "0.7176196584938573\n",
      "0.7176090168230379\n",
      "0.7176052670511347\n",
      "0.7175779082846632\n",
      "0.7178457366487111\n",
      "0.7179551680399634\n",
      "0.7179650241575571\n",
      "0.7178104682236692\n",
      "0.7177887547851597\n",
      "0.717785968154668\n",
      "0.7148446960934627\n",
      "0.7149688037144193\n",
      "0.7156476989917087\n",
      "0.7161612027095435\n",
      "0.7166501109273471\n",
      "0.7239279854512886\n",
      "0.7239272042237009\n",
      "0.723930959539793\n",
      "0.723930959539793\n",
      "0.7239218864540492\n",
      "0.7249204719276067\n",
      "0.7248781190568432\n",
      "0.7249464344297181\n",
      "0.7256399570038511\n",
      "0.7260931026708923\n",
      "0.7275070457521197\n",
      "0.7282039836917235\n",
      "0.7286573025886407\n",
      "0.7290586780441466\n",
      "0.7291242842606703\n",
      "0.7239000342576795\n",
      "0.7239138068963643\n",
      "0.7240917824774973\n",
      "0.724350386012348\n",
      "0.7241095159426546\n",
      "0.724350386012348\n",
      "0.7241095159426546\n",
      "0.7236954091091627\n",
      "0.7233605453113974\n",
      "0.7228787502008828\n",
      "0.71596056836145\n",
      "0.7158891542437753\n",
      "0.7159059649792683\n",
      "0.7157800775321561\n",
      "0.7148565352237319\n",
      "0.6604887835425616\n",
      "0.6590595860196665\n",
      "0.6584514790843565\n",
      "0.6585984053396148\n",
      "0.6586037257951222\n",
      "0.7169853333962938\n",
      "0.7169831588345444\n",
      "0.7169900541430405\n",
      "0.7169916277316236\n",
      "0.716996162662001\n",
      "0.707737516204199\n",
      "0.7086396572488889\n",
      "0.7089757521733822\n",
      "0.7087652119103106\n",
      "0.7087545696055395\n",
      "0.7228916241236059\n",
      "0.722688818789527\n",
      "0.7224117048033162\n",
      "0.7223238596698571\n",
      "0.7225511760110247\n",
      "0.7163969699289616\n",
      "0.7163770580491071\n",
      "0.7163877040140603\n",
      "0.7163731151452368\n",
      "0.7163263914556683\n",
      "0.7306322180787412\n",
      "0.7305383516248883\n",
      "0.7305757147682339\n",
      "0.7306451624672831\n",
      "0.7306783213085194\n",
      "0.7479958898864765\n",
      "0.7479960758359524\n",
      "0.7478994591631111\n",
      "0.7471401148130022\n",
      "0.7470456096845755\n",
      "0.7156740156807029\n",
      "0.7157491172202715\n",
      "0.7159319285501433\n",
      "0.7160434628635135\n",
      "0.7162094042266478\n",
      "0.7131704435096001\n",
      "0.7123684623997156\n",
      "0.71219390019836\n",
      "0.7117663480831294\n",
      "0.7117041736295415\n",
      "0.7221099379575289\n",
      "0.7221061851724428\n",
      "0.7220396154487863\n",
      "0.7221086712315675\n",
      "0.7217622092566688\n",
      "0.7204617100642675\n",
      "0.720439021926814\n",
      "0.7205742369237279\n",
      "0.7205301593254813\n",
      "0.7205096758555315\n",
      "0.7277663089657181\n",
      "0.7277810728403133\n",
      "0.7277300723804724\n",
      "0.7277488124969179\n",
      "0.7277638046137868\n",
      "0.7271678941668419\n",
      "0.7266124328251795\n",
      "0.7265261899180595\n",
      "0.7267225971920606\n",
      "0.726819837014993\n",
      "0.7158539018224662\n",
      "0.7158917551778591\n",
      "0.7159120724798789\n",
      "0.7158776049674693\n",
      "0.7158509995339662\n",
      "0.7218388903356286\n",
      "0.7224123173863574\n",
      "0.7227559614986593\n",
      "0.7229406640159941\n",
      "0.722902661127983\n",
      "0.7175094921906522\n",
      "0.7174340308309286\n",
      "0.7174245524710217\n",
      "0.7182144229517704\n",
      "0.7186208435871528\n",
      "0.7672834568763467\n",
      "0.767095786987735\n",
      "0.7670752847941292\n",
      "0.7670790300696676\n",
      "0.767072918614909\n",
      "0.713631285118615\n",
      "0.7137088647939093\n",
      "0.7139465733916828\n",
      "0.7142564448713575\n",
      "0.7147398335827129\n",
      "0.7121292045186125\n",
      "0.7118882423177716\n",
      "0.711883410102926\n",
      "0.7118932666056706\n",
      "0.7118895177136734\n",
      "0.716404854310498\n",
      "0.7164994319120533\n",
      "0.7164889718972147\n",
      "0.7164822606681023\n",
      "0.7164867957467271\n",
      "0.7369312914830377\n",
      "0.7367628513991377\n",
      "0.7365297104938772\n",
      "0.7360389269772202\n",
      "0.7359970171172833\n",
      "0.7169776153913977\n",
      "0.7165914975637824\n",
      "0.716649413778707\n",
      "0.7171049692014915\n",
      "0.7173492724502842\n",
      "0.7257873765952036\n",
      "0.7258466258288402\n",
      "0.7259700807694699\n",
      "0.7264920612178796\n",
      "0.7263542758594563\n",
      "0.7205524227914885\n",
      "0.7206304432744507\n",
      "0.7208517617408053\n",
      "0.7211139221826426\n",
      "0.7211168797081533\n",
      "0.7211139221826426\n",
      "0.7211168797081533\n",
      "0.7211078108902068\n",
      "0.7211139221826426\n",
      "0.7211154990692856\n",
      "0.7228805929638545\n",
      "0.7227371193578422\n",
      "0.7222051824395538\n",
      "0.7218295209451057\n",
      "0.7216162582838203\n",
      "0.7171941992673596\n",
      "0.7167549136942115\n",
      "0.7168680199999025\n",
      "0.716973874247116\n",
      "0.7169621092880213\n",
      "0.7294276503226083\n",
      "0.7294147528883728\n",
      "0.7293259729486771\n",
      "0.7291006037130499\n",
      "0.7288734103750134\n",
      "0.719998778702612\n",
      "0.7197830475488991\n",
      "0.7195302887714282\n",
      "0.719178116386628\n",
      "0.7183429122068876\n",
      "0.7159433363628005\n",
      "0.7159181204909959\n",
      "0.7158742789939835\n",
      "0.716827627328303\n",
      "0.7173321191432815\n",
      "0.7260162247858307\n",
      "0.7262178828107632\n",
      "0.7265604831724568\n",
      "0.7265514141505621\n",
      "0.7265468796396698\n",
      "0.7221565201504256\n",
      "0.7214676948026623\n",
      "0.7213626388222393\n",
      "0.7213416073530369\n",
      "0.7221083740452077\n",
      "0.7365382848815936\n",
      "0.7373790919293666\n",
      "0.7375724893037261\n",
      "0.7376147070329868\n",
      "0.7373722281226314\n",
      "0.7293474740384962\n",
      "0.7292598205948302\n",
      "0.7291508692387\n",
      "0.7291449881093698\n",
      "0.7295813092144054\n",
      "0.7346736324879845\n",
      "0.7346977088931762\n",
      "0.7347052115359665\n",
      "0.7347134984290686\n",
      "0.7347097471054171\n",
      "0.7193430630286259\n",
      "0.7194740681250678\n",
      "0.7194941473349749\n",
      "0.7192775406337395\n",
      "0.7187345680753051\n",
      "0.7245416565556526\n",
      "0.7254633872029206\n",
      "0.7268810490754236\n",
      "0.7286438368085766\n",
      "0.7304040909877861\n",
      "0.7218895111720044\n",
      "0.7219433028445197\n",
      "0.721633749169864\n",
      "0.7211899468927231\n",
      "0.7210517341767907\n",
      "0.7145209792591332\n",
      "0.714024953772671\n",
      "0.7138130060947531\n",
      "0.7137659078255579\n",
      "0.713746807683271\n",
      "0.7155864559031802\n",
      "0.7155795627793691\n",
      "0.7156966916071495\n",
      "0.7158804694924366\n",
      "0.715990231632997\n",
      "0.7167447974533365\n",
      "0.7170361223652517\n",
      "0.7174682555942423\n",
      "0.7174396579712893\n",
      "0.7174396579712893\n",
      "0.7264805876144335\n",
      "0.7263305684453277\n",
      "0.7262296169788073\n",
      "0.7257661005117656\n",
      "0.7255289191810315\n",
      "0.7342826501773687\n",
      "0.7339790498734153\n",
      "0.7336642718040555\n",
      "0.7333783161462473\n",
      "0.7332180663924645\n",
      "0.7174005554767091\n",
      "0.7174165209859497\n",
      "0.7174218428230582\n",
      "0.7174833835897069\n",
      "0.7175366417876313\n",
      "0.7170099649509888\n",
      "0.7170237507045584\n",
      "0.7170343928927767\n",
      "0.7170336069976959\n",
      "0.7170248970059327\n",
      "0.7299389222550975\n",
      "0.729907928541475\n",
      "0.729954257701111\n",
      "0.7299658401405386\n",
      "0.7299506112753645\n",
      "0.732644407967392\n",
      "0.7326273464726218\n",
      "0.7325725835775324\n",
      "0.7328271896415769\n",
      "0.7328855489774395\n",
      "0.7401550622033433\n",
      "0.7404508375767707\n",
      "0.7406333465557635\n",
      "0.7405645399461855\n",
      "0.7406008169228141\n",
      "0.7370404809340636\n",
      "0.7373485958148441\n",
      "0.737730468515847\n",
      "0.7381078295664869\n",
      "0.7384193212452504\n",
      "0.7159268869574417\n",
      "0.7159209611538833\n",
      "0.7158916334627207\n",
      "0.7158920566409107\n",
      "0.7162055866286308\n",
      "0.7231587907473337\n",
      "0.7236067238733432\n",
      "0.7238710829669341\n",
      "0.7237874121661211\n",
      "0.7238148097616282\n",
      "0.7275936426402289\n",
      "0.7276610203942488\n",
      "0.7276746249936807\n",
      "0.7276746249936807\n",
      "0.7276783730996556\n",
      "0.7317952042171988\n",
      "0.7317739223951735\n",
      "0.7317427847981198\n",
      "0.7317358940958463\n",
      "0.7317358940958463\n",
      "0.7163957765637025\n",
      "0.7164677765991389\n",
      "0.71651892122983\n",
      "0.7167045347813604\n",
      "0.7167015713944291\n",
      "0.44661291532581743\n",
      "0.44590222497276855\n",
      "0.4451776073830462\n",
      "0.4445137972935667\n",
      "0.44418257715572346\n",
      "0.7148256152632838\n",
      "0.7149624750515826\n",
      "0.7151640503421912\n",
      "0.7146918216027626\n",
      "0.7144639487745436\n",
      "0.7360131887236223\n",
      "0.7362645540996721\n",
      "0.7362856372954225\n",
      "0.7363067205002619\n",
      "0.7363157871460254\n",
      "0.717798697694485\n",
      "0.7177971221172733\n",
      "0.7178038280143076\n",
      "0.7178113217040186\n",
      "0.7177925874896167\n",
      "0.7335603329129291\n",
      "0.7336621901910696\n",
      "0.7337873998811533\n",
      "0.733853208537182\n",
      "0.7338812046388097\n",
      "0.7257759624758503\n",
      "0.7258145988300237\n",
      "0.7263956982547218\n",
      "0.7267580161331402\n",
      "0.7267744192620494\n",
      "0.7146416897003044\n",
      "0.7150437856947494\n",
      "0.7153297767593424\n",
      "0.7153214938869034\n",
      "0.7153161720547939\n",
      "0.7253017084597123\n",
      "0.725244932320547\n",
      "0.725357050085526\n",
      "0.7256915443953289\n",
      "0.72567698863237\n",
      "0.7085733776170928\n",
      "0.7083634693892088\n",
      "0.7082543525492266\n",
      "0.7078981213434106\n",
      "0.7076571643801934\n",
      "0.7333640023106616\n",
      "0.7333685379495668\n",
      "0.733418767268698\n",
      "0.733190800556976\n",
      "0.7329063455758094\n",
      "0.7072480141739859\n",
      "0.7069665829386905\n",
      "0.7070202465212616\n",
      "0.7070383944339519\n",
      "0.7070527855159651\n",
      "0.7174701980486191\n",
      "0.7174636652377891\n",
      "0.7169495507831813\n",
      "0.7167227269694882\n",
      "0.7168374259670879\n",
      "0.7244049632106279\n",
      "0.7248276376199344\n",
      "0.7252383696609512\n",
      "0.7253089411228901\n",
      "0.7252998716378636\n",
      "0.7171052231252611\n",
      "0.7169538641238458\n",
      "0.7164230121092265\n",
      "0.7160483909325581\n",
      "0.7161368248211939\n",
      "0.7332810699348989\n",
      "0.7332719989540362\n",
      "0.7332666789192106\n",
      "0.733265109832668\n",
      "0.7332763626843559\n",
      "0.7232363193460185\n",
      "0.7231624080410388\n",
      "0.7216973231614093\n",
      "0.7211919033793415\n",
      "0.7211079864658977\n",
      "0.7226615097289036\n",
      "0.7228929562614941\n",
      "0.7230647184862837\n",
      "0.7232445995198249\n",
      "0.7232511119767622\n",
      "0.7148586011507327\n",
      "0.7149388457298173\n",
      "0.7149879566356462\n",
      "0.7150894794335309\n",
      "0.715108905489723\n",
      "0.7400833189685131\n",
      "0.7394050487742427\n",
      "0.739482624608423\n",
      "0.7395229138438058\n",
      "0.7395734243796019\n",
      "0.681129566325903\n",
      "0.6810540842674374\n",
      "0.6812819316236364\n",
      "0.6813327407069996\n",
      "0.6809926922105002\n",
      "0.6875872522198743\n",
      "0.6884904007689779\n",
      "0.6895613216561423\n",
      "0.690820437991514\n",
      "0.6921304531066845\n",
      "0.7135366756213467\n",
      "0.7135374621501718\n",
      "0.7135609238478087\n",
      "0.7135715669735283\n",
      "0.7135826283787553\n",
      "0.7095624818804502\n",
      "0.7092393597591093\n",
      "0.7093896980157032\n",
      "0.7094416578675975\n",
      "0.7098418328854391\n",
      "0.7231833418084924\n",
      "0.7232359704817863\n",
      "0.723224736190336\n",
      "0.723201152472645\n",
      "0.7233500357349868\n",
      "0.7263596667622452\n",
      "0.7270755389699859\n",
      "0.727561063733029\n",
      "0.7282087235175194\n",
      "0.7292355458934948\n",
      "0.7175730204568294\n",
      "0.7175041642188301\n",
      "0.7175766340901917\n",
      "0.7177706197171022\n",
      "0.718079366479739\n",
      "0.7341277598170605\n",
      "0.7337101150904822\n",
      "0.733633352570478\n",
      "0.7336303858271469\n",
      "0.7336447768198022\n",
      "0.722824857471602\n",
      "0.7230118571641154\n",
      "0.7230781445967183\n",
      "0.723070469379785\n",
      "0.7230848606479366\n",
      "0.7155860946096978\n",
      "0.715020118994154\n",
      "0.7144387923849623\n",
      "0.7142203916458655\n",
      "0.7142211765868108\n",
      "0.7146853601636848\n",
      "0.7150782531077267\n",
      "0.7155778119258502\n",
      "0.7158466806289346\n",
      "0.7158821774178273\n",
      "0.731876966549934\n",
      "0.7329309360294011\n",
      "0.733387795230686\n",
      "0.7338803512859374\n",
      "0.7343126949546201\n",
      "0.7220426588832085\n",
      "0.7220389116613478\n",
      "0.7219998645526942\n",
      "0.7214943419941392\n",
      "0.7209263380717392\n",
      "0.7165921256473258\n",
      "0.7163620995890531\n",
      "0.7157411355208557\n",
      "0.7159123833771504\n",
      "0.7159465937273347\n",
      "0.7137267236366414\n",
      "0.7137048346414181\n",
      "0.7136135046167761\n",
      "0.713584067790261\n",
      "0.713635968977734\n",
      "0.7067925604308671\n",
      "0.7060163741446224\n",
      "0.7059842658666066\n",
      "0.7059773726216242\n",
      "0.7059903731141491\n",
      "0.7333497799948313\n",
      "0.7333497799948313\n",
      "0.7333166821624907\n",
      "0.73292952653011\n",
      "0.7328276179052224\n",
      "0.7326956388101172\n",
      "0.7332825031038848\n",
      "0.7338002425795912\n",
      "0.7340977523465836\n",
      "0.7340999419805657\n",
      "0.7148669371791888\n",
      "0.714901989522571\n",
      "0.7150627631201907\n",
      "0.7150895273583359\n",
      "0.715050937958879\n",
      "0.7204012071810798\n",
      "0.7202819020889869\n",
      "0.7201099387958386\n",
      "0.718039160153801\n",
      "0.7201136854690243\n",
      "0.7146942523111532\n",
      "0.7146973948009525\n",
      "0.7147805297152802\n",
      "0.7148469164837977\n",
      "0.715271091732445\n",
      "0.7215043440085429\n",
      "0.721800084191048\n",
      "0.7221611835498375\n",
      "0.7225473590248735\n",
      "0.7228047702835256\n",
      "0.7225301080375226\n",
      "0.722667751127327\n",
      "0.7231645581682471\n",
      "0.7234099659903843\n",
      "0.7243026486080568\n",
      "0.7324847867989663\n",
      "0.732322570680897\n",
      "0.7325882379222693\n",
      "0.7329869607291633\n",
      "0.7330035175984227\n",
      "0.7189558592211932\n",
      "0.7186247292236273\n",
      "0.7183145532527038\n",
      "0.718318895242002\n",
      "0.7183194905247914\n",
      "0.7157049760070223\n",
      "0.7159968130462729\n",
      "0.7161229870123281\n",
      "0.7162075850210814\n",
      "0.7162028703884892\n",
      "0.724842217284869\n",
      "0.7247890326541622\n",
      "0.7247714926426951\n",
      "0.7247675569706558\n",
      "0.7249218107950173\n",
      "0.7150451894229674\n",
      "0.7150459730990293\n",
      "0.7150353339974324\n",
      "0.7150406535480553\n",
      "0.7150406535480553\n",
      "0.7155986701988298\n",
      "0.7156763736035127\n",
      "0.715754863052395\n",
      "0.7157765724241794\n",
      "0.7157765724241794\n",
      "0.7217592893661986\n",
      "0.7217728937422568\n",
      "0.7217386006899892\n",
      "0.7217423483437311\n",
      "0.7217431354840288\n",
      "0.7220262852181976\n",
      "0.7220302226192167\n",
      "0.7219274997408669\n",
      "0.7213127641990834\n",
      "0.720916671683232\n",
      "0.7144749547637902\n",
      "0.7138948328124798\n",
      "0.7135860842537899\n",
      "0.7134851182031808\n",
      "0.713492430759089\n",
      "0.7307396509945121\n",
      "0.7303642418143843\n",
      "0.7295846853380381\n",
      "0.728609342310374\n",
      "0.7279252490001947\n",
      "0.715851863204779\n",
      "0.7154893978291562\n",
      "0.7153037548792572\n",
      "0.71495671695652\n",
      "0.714846618707167\n",
      "0.72992691933566\n",
      "0.7296252994450507\n",
      "0.7289006276802806\n",
      "0.7285666361113174\n",
      "0.7282744763249971\n",
      "0.7218227401052625\n",
      "0.7214711790431538\n",
      "0.7213187262233713\n",
      "0.7212465090747255\n",
      "0.7212240274276673\n",
      "0.7158502354228774\n",
      "0.7158494535131573\n",
      "0.7158702439857164\n",
      "0.7159019836775119\n",
      "0.7157267865609601\n",
      "0.7145505930968138\n",
      "0.7148849627802274\n",
      "0.7153000773010275\n",
      "0.7155579565036551\n",
      "0.7155640597307543\n",
      "0.7319985819970742\n",
      "0.7319341220716348\n",
      "0.7311142360818869\n",
      "0.7301478915287397\n",
      "0.7293697426354923\n",
      "0.734774767604583\n",
      "0.7344313028392552\n",
      "0.7342615411248955\n",
      "0.7340077925878362\n",
      "0.7339963686441806\n",
      "0.7133019043793798\n",
      "0.7133158751557553\n",
      "0.7136417435441993\n",
      "0.7135333219353875\n",
      "0.7135181440140677\n",
      "0.7674226092895392\n",
      "0.7678523716565188\n",
      "0.7672160372227497\n",
      "0.767163797020926\n",
      "0.7671385625062843\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot pack empty tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5563/1288058560.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/private/kaggle/data_module.py\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mcoord_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFromCoords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mcoord_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# feature for decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m#coord_list = coord_list[:-30,:,:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/data_module.py\u001b[0m in \u001b[0;36mtensorFromCoords\u001b[0;34m(coord_list)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_sequence\u001b[0;34m(sequences, enforce_sorted)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[1;32m    397\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menforce_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_packed_sequence_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot pack empty tensors."
     ]
    }
   ],
   "source": [
    "it = iter(train_dl)\n",
    "fs = next(it)\n",
    "fs[1][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82379ba",
   "metadata": {},
   "source": [
    "**Model Building**\n",
    "- Encoder-Decoder architecture\n",
    "    - Encoder -> MLP or CNN\n",
    "    - Decoder -> LSTM RNN\n",
    "    - Batch Normalization in both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6622884",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model Architectures \"\"\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(594, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512,256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, 256)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.dropout(F.relu(self.bn1(self.fc1(input.float()))))\n",
    "        x = self.dropout(F.relu(self.bn2(self.fc2(x))))\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = x.view(2, -1, 128)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845ec1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(1, 128, num_layers=2)\n",
    "        \n",
    "    def forward(self, in_feats, hidden):\n",
    "        do, hidden = self.gru(in_feats.to(torch.float), hidden.to(torch.float))\n",
    "        do = torch.sigmoid(do)\n",
    "        \n",
    "        return do, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081c2988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pred(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pred, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(256+594, 512) # Take in 128 feats from gru hidden plus categoricals\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc7 = nn.Linear(256, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "    def forward(self, in_feats):\n",
    "        x = self.dropout(F.relu(self.fc1(in_feats)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = (self.sig(self.fc7(x)) * 2 * 716.4264615618442) % (716.4264615618442 + 2*684.7511617508213)\n",
    "        x = x - (x % 15)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62bbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSE, self).__init__()\n",
    "        \n",
    "        self.crit = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        return torch.sqrt(self.crit(x.squeeze(0).to(torch.float64), y.squeeze(0).to(torch.float64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ede66",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "313b17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer helper functions from \n",
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#the-seq2seq-model\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s' % (asMinutes(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cafa9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(in_feats, cl, tt, encoder, decoder, pred, enc_optim, dec_optim, pred_optim, criterion):\n",
    "    enc_optim.zero_grad()\n",
    "    dec_optim.zero_grad()\n",
    "    pred_optim.zero_grad()\n",
    "    \n",
    "    in_feats = in_feats.to(device)\n",
    "    cl = cl.to(device)\n",
    "    tt = tt.to(device)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    hidden = encoder(in_feats)\n",
    "    \n",
    "    \n",
    "    di = cl[0].unsqueeze(0).to(device)\n",
    "    dh = hidden.to(device)\n",
    "                    \n",
    "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for i in range(len(cl) - 1):\n",
    "            do, dh = decoder(di, dh)\n",
    "            di = cl[i + 1].unsqueeze(0).to(device)  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for i in range(len(cl)):\n",
    "            do, dh = decoder(di, dh)\n",
    "            \n",
    "            topv, topi = do.topk(1, dim=2)\n",
    "            di = topv.detach().to(device)  # detach from history as input\n",
    "\n",
    "\n",
    "    dh = dh.reshape((1, 64, 256))\n",
    "    pred_in = torch.cat((in_feats.squeeze(0), dh.squeeze(0)), dim=1).to(device)\n",
    "\n",
    "    pred_time = pred(pred_in)\n",
    "    loss += criterion(pred_time, tt.unsqueeze(0).unsqueeze(-1))\n",
    "    loss.backward()\n",
    "        \n",
    "    enc_optim.step()\n",
    "    dec_optim.step()\n",
    "    pred_optim.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb3f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpochs(encoder, decoder, predictor, n_epochs, print_every=1000, eval_every = 5, learning_rate=0.003):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    enc_optim = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    dec_optim = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    pred_optim = optim.Adam(predictor.parameters(), lr=learning_rate)\n",
    "    criterion = RMSE()\n",
    "    es = optim.lr_scheduler.ReduceLROnPlateau(enc_optim, 'min', 0.25, 3)\n",
    "    ds = optim.lr_scheduler.ReduceLROnPlateau(dec_optim, 'min', 0.25, 3)\n",
    "    ps = optim.lr_scheduler.ReduceLROnPlateau(pred_optim, 'min', 0.25, 3)\n",
    "\n",
    "    \n",
    "    epoch_loss_max = math.inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for i, data in enumerate(train_dl):\n",
    "\n",
    "            in_feats = data[0] # back to data\n",
    "\n",
    "            if (in_feats.shape[0] != 64):\n",
    "                continue\n",
    "\n",
    "            cl = data[1] # back to data\n",
    "            tt = data[2] # back to data\n",
    "            loss = train(in_feats, cl, tt, encoder, decoder, pred, enc_optim, dec_optim, pred_optim, criterion)\n",
    "            print_loss_total += loss\n",
    "\n",
    "\n",
    "            if (i % 1000 == 0):\n",
    "                if (print_loss_total < epoch_loss_max):\n",
    "                    epoch_loss_max = print_loss_total\n",
    "                    torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'encoder_state_dict': encoder.state_dict(),\n",
    "                            'encoder_optimizer_state_dict': enc_optim.state_dict(),\n",
    "                            'decoder_state_dict': decoder.state_dict(),\n",
    "                            'decoder_optimizer_state_dict': dec_optim.state_dict(),\n",
    "                            'loss': print_loss_total,\n",
    "                            }, 'model.pt')\n",
    "\n",
    "            if (i % print_every == 0) and (i != 0): # Change back to i\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "\n",
    "\n",
    "\n",
    "                print('Epoch: %d Elapsed: %s Percent of epoch Complete: (%d%%) %.4f' % (epoch, timeSince(start, i / (train_len / 128)),\n",
    "                                                                  i / (train_len / 64) * 100, print_loss_avg))\n",
    "\n",
    "\n",
    "            if (i % eval_every == 0) and (i != 0):\n",
    "                print('*****EVALUATING*****')\n",
    "                eval_loss = eval_epoch(encoder, decoder, pred, epoch)\n",
    "                es.step(eval_loss)\n",
    "                ds.step(eval_loss)\n",
    "                ps.step(eval_loss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "258893e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, pred, in_feats, cl, tt, max_len=1500):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        eval_loss = RMSE()\n",
    "        in_feats = in_feats.to(device)\n",
    "        cl = cl.to(device)\n",
    "        tt = tt.to(device)\n",
    "\n",
    "        hidden = encoder(in_feats)\n",
    "\n",
    "\n",
    "        di = torch.zeros((1,64,1)).to(device)\n",
    "        dh = hidden.to(device)\n",
    "\n",
    "        for i in range(1000):\n",
    "            do, dh = dec(di, dh)\n",
    "\n",
    "            topv, topi = do.topk(1, dim=2)\n",
    "            di = topv.detach().to(device)  # detach from history as input\n",
    "            \n",
    "        dh = dh.reshape((1, 64, 256))\n",
    "\n",
    "        pred_in = torch.cat((in_feats.squeeze(0), dh.squeeze(0)), dim=1).to(device)\n",
    "        pred_time = pred(pred_in)\n",
    "        \n",
    "        l = eval_loss(pred_time, tt.unsqueeze(0).unsqueeze(-1))\n",
    "        \n",
    "        return l\n",
    "    \n",
    "def eval_epoch(encoder, decoder, pred, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    pred.eval()\n",
    "\n",
    "    accs = []\n",
    "    for i, data in enumerate(test_dl):\n",
    "        in_feats = data[0]\n",
    "        if (in_feats.shape[0] != 64):\n",
    "            continue\n",
    "        cl = data[1]\n",
    "        tt = data[2]\n",
    "        accs.append(evaluate(encoder, decoder, pred, in_feats, cl, tt))\n",
    "        \n",
    "        if (i > 100):\n",
    "            break\n",
    "    \n",
    "    epoch_acc = (sum(accs) / len(accs)) if len(accs) > 0 else 0\n",
    "    print('Epoch: %d, Loss on test: %.4f' % (epoch, epoch_acc))\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    pred.train()\n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b391701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7143064609935832\n",
      "0.7138260671656798\n",
      "0.7135311098847475\n",
      "0.7135201085461105\n",
      "0.7135793998444172\n",
      "0.7157057509144855\n",
      "0.7157012155392386\n",
      "0.7156974655008054\n",
      "0.7156391143139951\n",
      "0.71550069703607\n",
      "0.7184567710920018\n",
      "0.7184703764316986\n",
      "0.7184780561465574\n",
      "0.7186238485992814\n",
      "0.7188435871212454\n",
      "0.7666922472521916\n",
      "0.7666839642338565\n",
      "0.766685343560527\n",
      "0.7666924416403339\n",
      "0.7667034832248594\n",
      "0.7323706474580705\n",
      "0.7325533753230091\n",
      "0.732563233054637\n",
      "0.7325829485178336\n",
      "0.7323401133521246\n",
      "0.7668836712102534\n",
      "0.7668560705153553\n",
      "0.7668613932407946\n",
      "0.7668606045220994\n",
      "0.766857647954023\n",
      "0.7159776816295788\n",
      "0.7159851796198943\n",
      "0.7159747782898499\n",
      "0.7159057837771866\n",
      "0.7158890340758429\n",
      "0.7227651717195462\n",
      "0.722762979231253\n",
      "0.7227592240599826\n",
      "0.7227562502364645\n",
      "0.7227578129316898\n",
      "0.7305073761906825\n",
      "0.7305392630694546\n",
      "0.7307578244115643\n",
      "0.7314857922167634\n",
      "0.7319544630461703\n",
      "0.7675529370771463\n",
      "0.7671533499241544\n",
      "0.7671239781115116\n",
      "0.7671135310528889\n",
      "0.767098351580878\n",
      "0.721667308843356\n",
      "0.7216217013783107\n",
      "0.721588840878164\n",
      "0.7220974612264571\n",
      "0.722360850686263\n",
      "0.7316424566345776\n",
      "0.7316952059737859\n",
      "0.7317291974353937\n",
      "0.7319117548132182\n",
      "0.7319829867086428\n",
      "0.7136498512833258\n",
      "0.7127569687805134\n",
      "0.7115247146204039\n",
      "0.7104976331812197\n",
      "0.7094936186410993\n",
      "0.7228796646791782\n",
      "0.7228461843542838\n",
      "0.7228491543144795\n",
      "0.7228288233269158\n",
      "0.7228205343299561\n",
      "0.7147344469548986\n",
      "0.7148267316963273\n",
      "0.7150185209681286\n",
      "0.7150108989815814\n",
      "0.714875891061155\n",
      "0.7249656581237012\n",
      "0.7250362463380416\n",
      "0.7248011373197044\n",
      "0.7241087442029998\n",
      "0.7241087442029998\n",
      "0.7163515654175001\n",
      "0.7163563525331259\n",
      "0.716353210420683\n",
      "0.7165954054568038\n",
      "0.7163042060138809\n",
      "0.7230500713963691\n",
      "0.723093310996227\n",
      "0.7230674042429128\n",
      "0.7229759436485803\n",
      "0.7229706186082638\n",
      "0.7254724822188823\n",
      "0.7254099753751676\n",
      "0.7254470316121175\n",
      "0.7254781795799364\n",
      "0.7254773912972375\n",
      "0.7137634603614711\n",
      "0.7132513869128325\n",
      "0.7132582802667505\n",
      "0.7132582802667505\n",
      "0.7132620293828829\n",
      "0.7232215601276368\n",
      "0.7232081437382235\n",
      "0.7232897003138417\n",
      "0.7232161343672837\n",
      "0.7233819359895021\n",
      "0.7410515494094634\n",
      "0.740549497985896\n",
      "0.7402934850563527\n",
      "0.7404876427701416\n",
      "0.7404738280876373\n",
      "0.72006108970564\n",
      "0.7206921873260451\n",
      "0.7213655824931713\n",
      "0.721394550116351\n",
      "0.7214444827843565\n",
      "0.7176293367560289\n",
      "0.7170232101139873\n",
      "0.7169655722582784\n",
      "0.7169383605756293\n",
      "0.7168711888688394\n",
      "0.7148789294491926\n",
      "0.7148781457634031\n",
      "0.7148818979518448\n",
      "0.7150930653925476\n",
      "0.7153083973288129\n",
      "0.7218705149402992\n",
      "0.7215197043101557\n",
      "0.7213926283362325\n",
      "0.7214055407719691\n",
      "0.7214055407719691\n",
      "0.7153336211814301\n",
      "0.7153304785138158\n",
      "0.7153235862412227\n",
      "0.7153204435948229\n",
      "0.7155238170922631\n",
      "0.7250648202138137\n",
      "0.7250423354108909\n",
      "0.7250226219413601\n",
      "0.7250234092105942\n",
      "0.7250234092105942\n",
      "0.7160066464286915\n",
      "0.7158945364413762\n",
      "0.7159344451540175\n",
      "0.7159516182440845\n",
      "0.7160711697563671\n",
      "0.7453056307797647\n",
      "0.7462389117911303\n",
      "0.7462597645040102\n",
      "0.7462589714617154\n",
      "0.7462528525301063\n",
      "0.7319561914817583\n",
      "0.7319532367566937\n",
      "0.731950282035799\n",
      "0.7319420037119024\n",
      "0.7319473273190832\n",
      "0.7246469444574788\n",
      "0.7244509717735028\n",
      "0.7240916078457448\n",
      "0.7238011998636825\n",
      "0.7238011998636825\n",
      "0.7170078222813154\n",
      "0.7169956052876295\n",
      "0.71665492785905\n",
      "0.7165779658570781\n",
      "0.7164338730888985\n",
      "0.71710188118222\n",
      "0.7177970399166061\n",
      "0.7181626266866143\n",
      "0.71858282027151\n",
      "0.7186386384000496\n",
      "0.7175891707556181\n",
      "0.7173842219448995\n",
      "0.7172002345717297\n",
      "0.7168487280522382\n",
      "0.7168236930318167\n",
      "0.7147229572003096\n",
      "0.7146871118324256\n",
      "0.7145749792942083\n",
      "0.714497215857646\n",
      "0.7144862513702601\n",
      "0.7148840827765487\n",
      "0.7149266392354988\n",
      "0.7150957489521081\n",
      "0.7151692033492282\n",
      "0.7147206042340657\n",
      "0.7123753536271278\n",
      "0.7118821824910762\n",
      "0.7115894446402998\n",
      "0.711544918529826\n",
      "0.7116292967555563\n",
      "0.7266559177411397\n",
      "0.7264165512103417\n",
      "0.7263725839343373\n",
      "0.7263286171706651\n",
      "0.726001347252425\n",
      "0.7137672845182839\n",
      "0.7141152183631897\n",
      "0.7143559876947619\n",
      "0.7145256127888396\n",
      "0.7150132013890083\n",
      "0.7156840526952003\n",
      "0.7156965109770425\n",
      "0.7159274586698415\n",
      "0.7160154656583863\n",
      "0.7161090909187783\n",
      "0.7271899729355727\n",
      "0.7272035789319118\n",
      "0.7272118643826031\n",
      "0.7272163997151208\n",
      "0.7272148292904888\n",
      "0.7161692221319833\n",
      "0.7162066478305213\n",
      "0.7163463161779521\n",
      "0.7167594137696257\n",
      "0.7170152819159665\n",
      "0.7293322563254493\n",
      "0.730206368490414\n",
      "0.7306745628021485\n",
      "0.7313767650405237\n",
      "0.7320826915383917\n",
      "0.7240103257093168\n",
      "0.7240103257093168\n",
      "0.7240193954997795\n",
      "0.7240439184361149\n",
      "0.7240570639919021\n",
      "0.7166782780678683\n",
      "0.7161172450580127\n",
      "0.7158968369900496\n",
      "0.7159674659170309\n",
      "0.7159735731002871\n",
      "0.722497525887731\n",
      "0.7222441386263949\n",
      "0.7216994441176365\n",
      "0.7216303553467278\n",
      "0.7218889870816592\n",
      "0.7287548490969297\n",
      "0.728993208449994\n",
      "0.728986313634836\n",
      "0.7291302268313569\n",
      "0.7298478492262638\n",
      "0.7228940244364885\n",
      "0.7226770240290438\n",
      "0.7226785941524184\n",
      "0.722610388051995\n",
      "0.7224989187679638\n",
      "0.7225319278912301\n",
      "0.7228835608534396\n",
      "0.7229359258778021\n",
      "0.7229274719683644\n",
      "0.722929039194356\n",
      "0.7175443821567072\n",
      "0.7174220625365209\n",
      "0.7174220625365209\n",
      "0.717427383612209\n",
      "0.7174497658772103\n",
      "0.7356198027224631\n",
      "0.7356069873702654\n",
      "0.7356016649409872\n",
      "0.7352458661817266\n",
      "0.7346826585553853\n",
      "0.7232103693804183\n",
      "0.723223975094181\n",
      "0.723257828623428\n",
      "0.7233992065053183\n",
      "0.7235946503289531\n",
      "0.7541739993871157\n",
      "0.7545510591892128\n",
      "0.7546480812252453\n",
      "0.7545827568489888\n",
      "0.7545872902294957\n",
      "0.7148817317348923\n",
      "0.7149773175207184\n",
      "0.7151049869923106\n",
      "0.7152933892041385\n",
      "0.7157592716238843\n",
      "0.7174689326257409\n",
      "0.717599415361866\n",
      "0.7183572691863749\n",
      "0.7188397713457276\n",
      "0.7188818995213784\n",
      "0.7257491159128004\n",
      "0.7257731830116597\n",
      "0.7257527493771754\n",
      "0.725487975602206\n",
      "0.724897309403791\n",
      "0.7137042459036452\n",
      "0.7139106879757386\n",
      "0.7139402565600139\n",
      "0.7144157841599079\n",
      "0.7145670520385358\n",
      "0.7151888499129962\n",
      "0.7151971347761871\n",
      "0.7152062053253674\n",
      "0.715198706147663\n",
      "0.7151912069752199\n",
      "0.7581440918957808\n",
      "0.7585909758215913\n",
      "0.7582734789106607\n",
      "0.7582621613788647\n",
      "0.7587481803916499\n",
      "0.7229597681833471\n",
      "0.7229573367466646\n",
      "0.7230718891292877\n",
      "0.7232490081502462\n",
      "0.7232491866668993\n",
      "0.728869621022496\n",
      "0.7287215726147109\n",
      "0.7285009223148647\n",
      "0.7284525496991865\n",
      "0.7284506037604084\n",
      "0.73204669197622\n",
      "0.7320504431600361\n",
      "0.7321649262129567\n",
      "0.7324632329368008\n",
      "0.732358280967276\n",
      "0.7305550715594312\n",
      "0.7299256203274263\n",
      "0.7299046154109513\n",
      "0.7302886154284003\n",
      "0.7306698222343407\n",
      "0.71562491462226\n",
      "0.715794957549271\n",
      "0.716050714746096\n",
      "0.7164202463823769\n",
      "0.7167487912357848\n",
      "0.7149077383095418\n",
      "0.7140211204449837\n",
      "0.7134584613914176\n",
      "0.7136181955218497\n",
      "0.7137367978230684\n",
      "0.7281789865425397\n",
      "0.7281851006871465\n",
      "0.728184310538149\n",
      "0.7282212963874515\n",
      "0.728463888528437\n",
      "0.7336974342904428\n",
      "0.7335858734515484\n",
      "0.7335965141441251\n",
      "0.7336002645071618\n",
      "0.7336013108271026\n",
      "0.7225191723081418\n",
      "0.7225146379241303\n",
      "0.722568456645308\n",
      "0.7230077035719005\n",
      "0.7231797107646654\n",
      "0.7291942904250884\n",
      "0.7296947332048662\n",
      "0.7296960731688867\n",
      "0.7296824714974373\n",
      "0.7297230706132769\n",
      "0.7298644218615193\n",
      "0.7295517641116395\n",
      "0.7293252332833369\n",
      "0.7288325393753508\n",
      "0.7284179753661367\n",
      "0.7182602583960314\n",
      "0.7178583807439706\n",
      "0.7171611133219087\n",
      "0.7170369109654953\n",
      "0.7170000246931176\n",
      "0.7170140707994873\n",
      "0.7170950986031198\n",
      "0.7176491058342213\n",
      "0.717831700010984\n",
      "0.7176842652932585\n",
      "0.7147125719378952\n",
      "0.7147460346032917\n",
      "0.7147043615180723\n",
      "0.7147151825143468\n",
      "0.714712218686733\n",
      "0.7155931053264034\n",
      "0.7157355116695802\n",
      "0.7159467456177878\n",
      "0.7160904545543643\n",
      "0.7164732096363401\n",
      "0.7219091707496431\n",
      "0.7219095512569221\n",
      "0.7219211734313228\n",
      "0.7217764443606657\n",
      "0.7211705884084348\n",
      "0.7175272034662472\n",
      "0.7169836418186435\n",
      "0.7169781423428802\n",
      "0.7168536549665235\n",
      "0.7167365632360625\n",
      "0.7012891906835417\n",
      "0.7013337559914369\n",
      "0.7014197412155535\n",
      "0.7014410273124224\n",
      "0.7014455624815176\n",
      "0.7243722894836377\n",
      "0.7243776134994212\n",
      "0.7243805670991502\n",
      "0.724373869762998\n",
      "0.724365592151977\n",
      "0.7216770439914687\n",
      "0.721674078428878\n",
      "0.721674863374408\n",
      "0.721677828935176\n",
      "0.7216770439914687\n",
      "0.7156385201759246\n",
      "0.7156491624974862\n",
      "0.7157894559482327\n",
      "0.7159434876978032\n",
      "0.7161535891168485\n",
      "0.7210711798940724\n",
      "0.721557808560178\n",
      "0.7222402678377562\n",
      "0.7223488743028471\n",
      "0.7225304211290711\n",
      "0.725260198031254\n",
      "0.7252858085086373\n",
      "0.7253471116612265\n",
      "0.7253492815032796\n",
      "0.7252059417440316\n",
      "0.7138550162701288\n",
      "0.7138039767170188\n",
      "0.7137994416661682\n",
      "0.7138031902516425\n",
      "0.7137964795494619\n",
      "0.7252467890861842\n",
      "0.7252489590872528\n",
      "0.7253603164037061\n",
      "0.7253823952641042\n",
      "0.7253900825791033\n",
      "0.7118212335904429\n",
      "0.7118299163703834\n",
      "0.7118320924921452\n",
      "0.7118358412721956\n",
      "0.711832695976008\n",
      "0.714807922869049\n",
      "0.7148396739726712\n",
      "0.7148729924563946\n",
      "0.7149200848317299\n",
      "0.714947466354904\n",
      "0.7220641916172545\n",
      "0.7220756214046901\n",
      "0.7220929745450146\n",
      "0.7221012578006718\n",
      "0.7220996845395766\n",
      "0.7358643275093749\n",
      "0.7362086665603531\n",
      "0.7371594760594544\n",
      "0.7379586859503003\n",
      "0.7385246770269245\n",
      "0.7320739762966338\n",
      "0.7329555711611755\n",
      "0.7334639492686615\n",
      "0.7339191159349888\n",
      "0.7338387979085299\n",
      "0.716511987195444\n",
      "0.716429568953912\n",
      "0.7159993373829923\n",
      "0.7158920566409107\n",
      "0.7158653340766554\n",
      "0.7287020283426268\n",
      "0.7287569465189793\n",
      "0.728759916438886\n",
      "0.728771175289212\n",
      "0.7287786811959989\n",
      "0.7148822304801279\n",
      "0.7150364501002051\n",
      "0.7151021848655926\n",
      "0.715114155949154\n",
      "0.7150316777555294\n",
      "0.6088136535614128\n",
      "0.6088045797442349\n",
      "0.6088023907343372\n",
      "0.6087986364612943\n",
      "0.6087828367434438\n",
      "0.7153377283068826\n",
      "0.7153377283068826\n",
      "0.7156391275569447\n",
      "0.7159097457454682\n",
      "0.7161549027968285\n",
      "0.7303935813378905\n",
      "0.7300937356759706\n",
      "0.7294015861321947\n",
      "0.7292361660682897\n",
      "0.7293327418142858\n",
      "0.7149138153050786\n",
      "0.7149676283373471\n",
      "0.7149895240079949\n",
      "0.7150023479381741\n",
      "0.7150993348628933\n",
      "0.7251628392522067\n",
      "0.724982440003\n",
      "0.7249530441268623\n",
      "0.7249311506087097\n",
      "0.724913008263673\n",
      "0.73301092705558\n",
      "0.7330408767437407\n",
      "0.7331669919150086\n",
      "0.7325603921399124\n",
      "0.7319111301984318\n",
      "0.7270668831124275\n",
      "0.7264902949399193\n",
      "0.7264485030064931\n",
      "0.7264423925427752\n",
      "0.7264370700559354\n",
      "0.7244539993339661\n",
      "0.7241525469341168\n",
      "0.7243837736222316\n",
      "0.7247175416678857\n",
      "0.7248015274792649\n",
      "0.7290542165457615\n",
      "0.7290542165457615\n",
      "0.7290617230716253\n",
      "0.7290783016944127\n",
      "0.7290790844732411\n",
      "0.7249593687287597\n",
      "0.7248673220432531\n",
      "0.7248483958099969\n",
      "0.7248310438327339\n",
      "0.7248103553686607\n",
      "0.7305201192436038\n",
      "0.7305246533676957\n",
      "0.730518540900352\n",
      "0.7305169625602721\n",
      "0.7305177517299145\n",
      "0.7164687643620586\n",
      "0.7164650159210127\n",
      "0.7161450215985236\n",
      "0.7155842547803166\n",
      "0.7151248767833928\n",
      "0.7056817856476949\n",
      "0.7058024271813893\n",
      "0.7057738288826563\n",
      "0.7057813254855807\n",
      "0.7057789653117486\n",
      "0.7220186006196281\n",
      "0.7219235623040421\n",
      "0.7213908317607967\n",
      "0.7211802543376883\n",
      "0.7215289196442387\n",
      "0.71847351230583\n",
      "0.7180017167744659\n",
      "0.7177149024563518\n",
      "0.7170907989019416\n",
      "0.7170198078827335\n",
      "0.7164664540207906\n",
      "0.716401390029562\n",
      "0.716013307119072\n",
      "0.7159242872652252\n",
      "0.7158223491939996\n",
      "0.7248879555490558\n",
      "0.7251332028854561\n",
      "0.7251227559777244\n",
      "0.7251194096820153\n",
      "0.725121187150209\n",
      "0.7198774386865274\n",
      "0.7198690123288897\n",
      "0.7196586259862038\n",
      "0.7191863659151022\n",
      "0.7185965153048426\n",
      "0.7012745948113068\n",
      "0.700487857601522\n",
      "0.699735401313652\n",
      "0.6990433875954908\n",
      "0.6986064087048709\n",
      "0.7156373096093298\n",
      "0.7157079392536403\n",
      "0.7158859708259551\n",
      "0.7160659594809786\n",
      "0.7162121886501828\n",
      "0.724357460547442\n",
      "0.7245413600554855\n",
      "0.7247722242292537\n",
      "0.7247895722459612\n",
      "0.7247712359691514\n",
      "0.7163731830983796\n",
      "0.7163450193941592\n",
      "0.7170640139456415\n",
      "0.7177950640537415\n",
      "0.7182664477912176\n",
      "0.7185209190190557\n",
      "0.7185206446449977\n",
      "0.7186741390269182\n",
      "0.7187187096119478\n",
      "0.7191222780979131\n",
      "0.7330371749132513\n",
      "0.7331025219028646\n",
      "0.7331652243109703\n",
      "0.73316601076848\n",
      "0.7331827613614956\n",
      "0.7236160160579728\n",
      "0.7236483555921726\n",
      "0.7236456231889105\n",
      "0.7236024481473339\n",
      "0.7235971248607515\n",
      "0.7247812604008136\n",
      "0.7247729764845852\n",
      "0.7246130308091683\n",
      "0.7245530476898417\n",
      "0.7245492986291691\n",
      "0.7270032105978126\n",
      "0.7267931219547956\n",
      "0.7266205785950625\n",
      "0.7260796237564421\n",
      "0.7255102003369562\n",
      "0.7281547752473188\n",
      "0.7281502414054352\n",
      "0.7281494512387336\n",
      "0.7281403835546112\n",
      "0.7281305257037661\n",
      "0.7470539775338765\n",
      "0.7458712293712297\n",
      "0.744028922219904\n",
      "0.7428926734580489\n",
      "0.7419214451889077\n",
      "0.7233869995304429\n",
      "0.7234061687402162\n",
      "0.723275281321015\n",
      "0.7231977407600225\n",
      "0.7230897872114852\n",
      "0.7160127343904453\n",
      "0.7160596397579748\n",
      "0.7161378174227183\n",
      "0.7161924210224753\n",
      "0.7162014912173958\n",
      "0.7258805377106862\n",
      "0.7258706801304018\n",
      "0.7258698907208616\n",
      "0.7256374189031459\n",
      "0.7252048241985605\n",
      "0.7210687528908684\n",
      "0.721045290211618\n",
      "0.7210271472776173\n",
      "0.7210362187445394\n",
      "0.7210407544780639\n",
      "0.7146879673695683\n",
      "0.7148135981715918\n",
      "0.7146397240472735\n",
      "0.7145690891691585\n",
      "0.7145653396226507\n",
      "0.732767194857068\n",
      "0.7327326792136762\n",
      "0.7327247820153362\n",
      "0.7327056521821654\n",
      "0.7327277365311575\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot pack empty tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5563/1216705115.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainEpochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.003\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5563/3922341990.py\u001b[0m in \u001b[0;36mtrainEpochs\u001b[0;34m(encoder, decoder, predictor, n_epochs, print_every, eval_every, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0min_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# back to data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/private/kaggle/data_module.py\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mcoord_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFromCoords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mcoord_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# feature for decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m#coord_list = coord_list[:-30,:,:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/data_module.py\u001b[0m in \u001b[0;36mtensorFromCoords\u001b[0;34m(coord_list)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_sequence\u001b[0;34m(sequences, enforce_sorted)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[1;32m    397\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menforce_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_packed_sequence_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot pack empty tensors."
     ]
    }
   ],
   "source": [
    "enc = Encoder().to(device)\n",
    "dec = Decoder().to(device)\n",
    "pred = Pred().to(device)\n",
    "\n",
    "trainEpochs(enc, dec, pred, 1, print_every=50, eval_every = 250, learning_rate = 0.003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5debfdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(enc, dec, pred):\n",
    "    test_dl, test_len = dm.get_loader(test=True)\n",
    "    trip_ids = []\n",
    "    pred_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc.eval()\n",
    "        dec.eval()\n",
    "        pred.eval()\n",
    "        for i, (trip_id, in_feats) in enumerate(test_dl):\n",
    "            in_feats = in_feats.to(device)\n",
    "            \n",
    "            hidden = enc(in_feats)\n",
    "\n",
    "            di = torch.Tensor([0]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            dh = hidden.to(device)\n",
    "                                \n",
    "            for i in range(200):\n",
    "                do, dh = dec(di, dh)\n",
    "            \n",
    "                topv, topi = do.topk(1, dim=2)\n",
    "                di = topv.detach().to(device)  # detach from history as input\n",
    "                if (topv == EOS):\n",
    "                    break\n",
    "            \n",
    "            dh = dh.reshape((1, 1, 256))\n",
    "            pred_in = torch.cat((in_feats, dh.squeeze(0)), dim=1).to(device)\n",
    "            pred_time = pred(pred_in)\n",
    "            trip_ids.append(trip_id[0])\n",
    "            pred_times.append(pred_time.item())\n",
    "\n",
    "    df_sample = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "    df_sample[\"TRAVEL_TIME\"] = pred_times\n",
    "    df_sample.to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13e64d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission(enc, dec, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68606be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747bc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
