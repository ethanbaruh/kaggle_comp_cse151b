{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ebde34",
   "metadata": {},
   "source": [
    "**Model Config**\n",
    "Use Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03d65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import data_module as dm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SOS = 0\n",
    "EOS = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58638407",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "- Load CSV file dataset\n",
    "- Create Torch Dataset\n",
    "- Create Torch DataLoader\n",
    "- Create padding func (with insertion of SOS and EOS tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d4005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl, train_len, test_len = dm.get_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "693bd733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7239],\n",
       "        [0.7259],\n",
       "        [0.7199],\n",
       "        [0.7330],\n",
       "        [0.7276],\n",
       "        [0.7138],\n",
       "        [0.7202],\n",
       "        [0.7176],\n",
       "        [0.7163],\n",
       "        [0.7205],\n",
       "        [0.7240],\n",
       "        [0.7218],\n",
       "        [0.7154],\n",
       "        [0.7202],\n",
       "        [0.7224],\n",
       "        [0.7154],\n",
       "        [0.7194],\n",
       "        [0.7226],\n",
       "        [0.7254],\n",
       "        [0.7186],\n",
       "        [0.7159],\n",
       "        [0.7138],\n",
       "        [0.7232],\n",
       "        [0.7137],\n",
       "        [0.7231],\n",
       "        [0.7148],\n",
       "        [0.7158],\n",
       "        [0.7175],\n",
       "        [0.7209],\n",
       "        [0.7163],\n",
       "        [0.7292],\n",
       "        [0.7228],\n",
       "        [0.7138],\n",
       "        [0.7272],\n",
       "        [0.7167],\n",
       "        [0.7232],\n",
       "        [0.7274],\n",
       "        [0.7143],\n",
       "        [0.7157],\n",
       "        [0.7236],\n",
       "        [0.7166],\n",
       "        [0.7163],\n",
       "        [0.7138],\n",
       "        [0.7240],\n",
       "        [0.7183],\n",
       "        [0.7269],\n",
       "        [0.7290],\n",
       "        [0.7176],\n",
       "        [0.7221],\n",
       "        [0.7290],\n",
       "        [0.7111],\n",
       "        [0.7175],\n",
       "        [0.7158],\n",
       "        [0.7317],\n",
       "        [0.7237],\n",
       "        [0.7148],\n",
       "        [0.7230],\n",
       "        [0.7122],\n",
       "        [0.7485],\n",
       "        [0.7228],\n",
       "        [0.7197],\n",
       "        [0.7149],\n",
       "        [0.7165],\n",
       "        [0.7165]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(train_dl)\n",
    "fs = next(it)\n",
    "fs[1][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82379ba",
   "metadata": {},
   "source": [
    "**Model Building**\n",
    "- Encoder-Decoder architecture\n",
    "    - Encoder -> MLP or CNN\n",
    "    - Decoder -> LSTM RNN\n",
    "    - Batch Normalization in both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "845ec1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(1, 128)\n",
    "        \n",
    "    def forward(self, in_feats, hidden):\n",
    "        do, hidden = self.gru(in_feats.to(torch.float), hidden.to(torch.float))\n",
    "        do = torch.sigmoid(do)\n",
    "        \n",
    "        return do, hidden\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        return nn.init.xavier_normal_(torch.ones((1, bs, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "081c2988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pred(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pred, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(594+128, 512) # Take in 594 feats from gru hidden plus categoricals\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc7 = nn.Linear(64, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "    def forward(self, in_feats):\n",
    "        x = self.dropout(F.relu(self.fc1(in_feats)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = (self.sig(self.fc7(x)) * 2 * 716.4264615618442) % (716.4264615618442 + 2*684.7511617508213)\n",
    "        x = x - (x % 15)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d62bbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSE, self).__init__()\n",
    "        \n",
    "        self.crit = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        return torch.sqrt(self.crit(x.squeeze(0).to(torch.float64), y.squeeze(0).to(torch.float64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ede66",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "313b17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer helper functions from \n",
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#the-seq2seq-model\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s' % (asMinutes(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cafa9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(in_feats, cl, tt, decoder, pred, dec_optim, pred_optim, criterion):\n",
    "    dec_optim.zero_grad()\n",
    "    pred_optim.zero_grad()\n",
    "    \n",
    "    in_feats = in_feats.to(device)\n",
    "    cl = cl.to(device)\n",
    "    tt = tt.to(device)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    di = cl[0].unsqueeze(0).to(device)\n",
    "    dh = decoder.init_hidden(64).to(device)\n",
    "                    \n",
    "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for i in range(len(cl) - 1):\n",
    "            do, dh = decoder(di, dh)\n",
    "            di = cl[i + 1].unsqueeze(0).to(device)  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for i in range(len(cl)):\n",
    "            do, dh = decoder(di, dh)\n",
    "            \n",
    "            topv, topi = do.topk(1, dim=2)\n",
    "            di = topv.detach().to(device)  # detach from history as input\n",
    "\n",
    "\n",
    "    pred_in = torch.cat((in_feats.squeeze(0), dh.squeeze(0)), dim=1).to(device)\n",
    "\n",
    "    pred_time = pred(pred_in)\n",
    "    loss += criterion(pred_time, tt.unsqueeze(0).unsqueeze(-1))\n",
    "    loss.backward()\n",
    "        \n",
    "    dec_optim.step()\n",
    "    pred_optim.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfb3f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpochs(decoder, predictor, n_epochs, print_every=1000, eval_every = 5, learning_rate=0.003):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    dec_optim = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    pred_optim = optim.Adam(predictor.parameters(), lr=learning_rate)\n",
    "    criterion = RMSE()\n",
    "    ds = optim.lr_scheduler.ReduceLROnPlateau(dec_optim, 'min', 0.25, 3)\n",
    "    ps = optim.lr_scheduler.ReduceLROnPlateau(pred_optim, 'min', 0.25, 3)\n",
    "\n",
    "    \n",
    "    epoch_loss_max = math.inf\n",
    "    \n",
    "    it = iter(train_dl)\n",
    "    d = next(it)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for i, data in enumerate(train_dl):\n",
    "            if (i > 1):\n",
    "                break\n",
    "\n",
    "            in_feats = d[0] # back to data\n",
    "\n",
    "            if (in_feats.shape[0] != 64):\n",
    "                continue\n",
    "\n",
    "            cl = d[1] # back to data\n",
    "            tt = d[2] # back to data\n",
    "            loss = train(in_feats, cl, tt, decoder, pred, dec_optim, pred_optim, criterion)\n",
    "            print_loss_total += loss\n",
    "\n",
    "            if (i % print_every == 0) and (i != 0): # Change back to i\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "\n",
    "\n",
    "\n",
    "                print('Epoch: %d Elapsed: %s Percent of epoch Complete: (%d%%) %.4f' % (epoch, timeSince(start, i / (train_len / 128)),\n",
    "                                                                  i / (train_len / 64) * 100, print_loss_avg))\n",
    "\n",
    "\n",
    "            if (i % eval_every == 0) and (i != 0):\n",
    "                print('*****EVALUATING*****')\n",
    "                eval_loss = eval_epoch(encoder, decoder, pred, epoch)\n",
    "                ds.step(eval_loss)\n",
    "                ps.step(eval_loss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "258893e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(decoder, pred, in_feats, cl, tt, max_len=1500):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        eval_loss = RMSE()\n",
    "        in_feats = in_feats.to(device)\n",
    "        cl = cl.to(device)\n",
    "        tt = tt.to(device)\n",
    "\n",
    "\n",
    "\n",
    "        di = torch.zeros((1,64,1)).to(device)\n",
    "        dh = decoder.init_hidden(in_feats).to(device)\n",
    "\n",
    "        for i in range(1000):\n",
    "            do, dh = dec(di, dh)\n",
    "\n",
    "            topv, topi = do.topk(1, dim=2)\n",
    "            di = topv.detach().to(device)  # detach from history as input\n",
    "            \n",
    "        pred_in = torch.cat((in_feats.squeeze(0), dh.squeeze(0)), dim=1).to(device)\n",
    "        pred_time = pred(pred_in)\n",
    "        \n",
    "        l = eval_loss(pred_time, tt.unsqueeze(0).unsqueeze(-1))\n",
    "        \n",
    "        return l\n",
    "    \n",
    "def eval_epoch(decoder, pred, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    pred.eval()\n",
    "\n",
    "    accs = []\n",
    "    for i, data in enumerate(test_dl):\n",
    "        in_feats = data[0]\n",
    "        if (in_feats.shape[0] != 64):\n",
    "            continue\n",
    "        cl = data[1]\n",
    "        tt = data[2]\n",
    "        accs.append(evaluate(decoder, pred, in_feats, cl, tt))\n",
    "        \n",
    "        if (i > 100):\n",
    "            break\n",
    "    \n",
    "    epoch_acc = (sum(accs) / len(accs)) if len(accs) > 0 else 0\n",
    "    print('Epoch: %d, Loss on test: %.4f' % (epoch, epoch_acc))\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    pred.train()\n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b391701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Elapsed: 0m 0s Percent of epoch Complete: (0%) 628.6849\n",
      "Epoch: 1 Elapsed: 0m 0s Percent of epoch Complete: (0%) 628.1146\n",
      "Epoch: 2 Elapsed: 0m 0s Percent of epoch Complete: (0%) 628.5464\n",
      "Epoch: 3 Elapsed: 0m 0s Percent of epoch Complete: (0%) 627.9798\n",
      "Epoch: 4 Elapsed: 0m 0s Percent of epoch Complete: (0%) 628.4510\n",
      "Epoch: 5 Elapsed: 0m 0s Percent of epoch Complete: (0%) 627.9649\n",
      "Epoch: 6 Elapsed: 0m 0s Percent of epoch Complete: (0%) 628.9603\n",
      "Epoch: 7 Elapsed: 0m 0s Percent of epoch Complete: (0%) 629.0403\n",
      "Epoch: 8 Elapsed: 0m 0s Percent of epoch Complete: (0%) 628.7487\n",
      "Epoch: 9 Elapsed: 0m 0s Percent of epoch Complete: (0%) 630.4184\n",
      "Epoch: 10 Elapsed: 0m 1s Percent of epoch Complete: (0%) 629.9383\n",
      "Epoch: 11 Elapsed: 0m 1s Percent of epoch Complete: (0%) 625.6654\n",
      "Epoch: 12 Elapsed: 0m 1s Percent of epoch Complete: (0%) 630.2950\n",
      "Epoch: 13 Elapsed: 0m 1s Percent of epoch Complete: (0%) 628.3554\n",
      "Epoch: 14 Elapsed: 0m 1s Percent of epoch Complete: (0%) 624.4341\n",
      "Epoch: 15 Elapsed: 0m 1s Percent of epoch Complete: (0%) 627.4625\n",
      "Epoch: 16 Elapsed: 0m 1s Percent of epoch Complete: (0%) 629.2904\n",
      "Epoch: 17 Elapsed: 0m 1s Percent of epoch Complete: (0%) 627.8143\n",
      "Epoch: 18 Elapsed: 0m 1s Percent of epoch Complete: (0%) 627.1751\n",
      "Epoch: 19 Elapsed: 0m 1s Percent of epoch Complete: (0%) 626.7703\n",
      "Epoch: 20 Elapsed: 0m 2s Percent of epoch Complete: (0%) 628.5250\n",
      "Epoch: 21 Elapsed: 0m 2s Percent of epoch Complete: (0%) 629.1116\n",
      "Epoch: 22 Elapsed: 0m 2s Percent of epoch Complete: (0%) 628.9765\n",
      "Epoch: 23 Elapsed: 0m 2s Percent of epoch Complete: (0%) 627.3493\n",
      "Epoch: 24 Elapsed: 0m 2s Percent of epoch Complete: (0%) 627.9870\n",
      "Epoch: 25 Elapsed: 0m 2s Percent of epoch Complete: (0%) 629.2740\n",
      "Epoch: 26 Elapsed: 0m 2s Percent of epoch Complete: (0%) 628.0557\n",
      "Epoch: 27 Elapsed: 0m 2s Percent of epoch Complete: (0%) 628.8996\n",
      "Epoch: 28 Elapsed: 0m 2s Percent of epoch Complete: (0%) 632.1111\n",
      "Epoch: 29 Elapsed: 0m 2s Percent of epoch Complete: (0%) 628.6758\n",
      "Epoch: 30 Elapsed: 0m 2s Percent of epoch Complete: (0%) 628.1464\n",
      "Epoch: 31 Elapsed: 0m 3s Percent of epoch Complete: (0%) 626.8501\n",
      "Epoch: 32 Elapsed: 0m 3s Percent of epoch Complete: (0%) 627.7582\n",
      "Epoch: 33 Elapsed: 0m 3s Percent of epoch Complete: (0%) 626.9513\n",
      "Epoch: 34 Elapsed: 0m 3s Percent of epoch Complete: (0%) 627.7669\n",
      "Epoch: 35 Elapsed: 0m 3s Percent of epoch Complete: (0%) 628.9044\n",
      "Epoch: 36 Elapsed: 0m 3s Percent of epoch Complete: (0%) 628.3067\n",
      "Epoch: 37 Elapsed: 0m 3s Percent of epoch Complete: (0%) 628.3843\n",
      "Epoch: 38 Elapsed: 0m 3s Percent of epoch Complete: (0%) 626.2774\n",
      "Epoch: 39 Elapsed: 0m 3s Percent of epoch Complete: (0%) 626.0232\n",
      "Epoch: 40 Elapsed: 0m 3s Percent of epoch Complete: (0%) 628.3609\n",
      "Epoch: 41 Elapsed: 0m 4s Percent of epoch Complete: (0%) 631.7943\n",
      "Epoch: 42 Elapsed: 0m 4s Percent of epoch Complete: (0%) 628.0716\n",
      "Epoch: 43 Elapsed: 0m 4s Percent of epoch Complete: (0%) 628.1831\n",
      "Epoch: 44 Elapsed: 0m 4s Percent of epoch Complete: (0%) 628.0098\n",
      "Epoch: 45 Elapsed: 0m 4s Percent of epoch Complete: (0%) 629.0727\n",
      "Epoch: 46 Elapsed: 0m 4s Percent of epoch Complete: (0%) 630.1710\n",
      "Epoch: 47 Elapsed: 0m 4s Percent of epoch Complete: (0%) 626.4910\n",
      "Epoch: 48 Elapsed: 0m 4s Percent of epoch Complete: (0%) 630.2836\n",
      "Epoch: 49 Elapsed: 0m 4s Percent of epoch Complete: (0%) 630.7498\n",
      "Epoch: 50 Elapsed: 0m 4s Percent of epoch Complete: (0%) 629.1443\n",
      "Epoch: 51 Elapsed: 0m 5s Percent of epoch Complete: (0%) 627.2540\n",
      "Epoch: 52 Elapsed: 0m 5s Percent of epoch Complete: (0%) 629.4417\n",
      "Epoch: 53 Elapsed: 0m 5s Percent of epoch Complete: (0%) 629.6420\n",
      "Epoch: 54 Elapsed: 0m 5s Percent of epoch Complete: (0%) 629.3188\n",
      "Epoch: 55 Elapsed: 0m 5s Percent of epoch Complete: (0%) 628.7125\n",
      "Epoch: 56 Elapsed: 0m 5s Percent of epoch Complete: (0%) 628.7987\n",
      "Epoch: 57 Elapsed: 0m 5s Percent of epoch Complete: (0%) 629.3159\n",
      "Epoch: 58 Elapsed: 0m 5s Percent of epoch Complete: (0%) 626.8053\n",
      "Epoch: 59 Elapsed: 0m 5s Percent of epoch Complete: (0%) 627.8763\n",
      "Epoch: 60 Elapsed: 0m 5s Percent of epoch Complete: (0%) 629.4567\n",
      "Epoch: 61 Elapsed: 0m 6s Percent of epoch Complete: (0%) 629.4416\n",
      "Epoch: 62 Elapsed: 0m 6s Percent of epoch Complete: (0%) 627.1288\n",
      "Epoch: 63 Elapsed: 0m 6s Percent of epoch Complete: (0%) 629.4579\n",
      "Epoch: 64 Elapsed: 0m 6s Percent of epoch Complete: (0%) 627.6630\n",
      "Epoch: 65 Elapsed: 0m 6s Percent of epoch Complete: (0%) 627.9709\n",
      "Epoch: 66 Elapsed: 0m 6s Percent of epoch Complete: (0%) 628.0290\n",
      "Epoch: 67 Elapsed: 0m 6s Percent of epoch Complete: (0%) 626.8102\n",
      "Epoch: 68 Elapsed: 0m 6s Percent of epoch Complete: (0%) 631.4657\n",
      "Epoch: 69 Elapsed: 0m 6s Percent of epoch Complete: (0%) 626.4786\n",
      "Epoch: 70 Elapsed: 0m 6s Percent of epoch Complete: (0%) 630.4516\n",
      "Epoch: 71 Elapsed: 0m 7s Percent of epoch Complete: (0%) 627.8482\n",
      "Epoch: 72 Elapsed: 0m 7s Percent of epoch Complete: (0%) 629.2291\n",
      "Epoch: 73 Elapsed: 0m 7s Percent of epoch Complete: (0%) 628.6915\n",
      "Epoch: 74 Elapsed: 0m 7s Percent of epoch Complete: (0%) 627.8473\n",
      "Epoch: 75 Elapsed: 0m 7s Percent of epoch Complete: (0%) 626.8223\n",
      "Epoch: 76 Elapsed: 0m 7s Percent of epoch Complete: (0%) 628.0146\n",
      "Epoch: 77 Elapsed: 0m 7s Percent of epoch Complete: (0%) 630.3848\n",
      "Epoch: 78 Elapsed: 0m 7s Percent of epoch Complete: (0%) 630.3068\n",
      "Epoch: 79 Elapsed: 0m 7s Percent of epoch Complete: (0%) 627.0009\n",
      "Epoch: 80 Elapsed: 0m 7s Percent of epoch Complete: (0%) 628.8773\n",
      "Epoch: 81 Elapsed: 0m 8s Percent of epoch Complete: (0%) 627.5664\n",
      "Epoch: 82 Elapsed: 0m 8s Percent of epoch Complete: (0%) 628.7990\n",
      "Epoch: 83 Elapsed: 0m 8s Percent of epoch Complete: (0%) 630.3903\n",
      "Epoch: 84 Elapsed: 0m 8s Percent of epoch Complete: (0%) 628.8323\n",
      "Epoch: 85 Elapsed: 0m 8s Percent of epoch Complete: (0%) 626.7433\n",
      "Epoch: 86 Elapsed: 0m 8s Percent of epoch Complete: (0%) 626.6255\n",
      "Epoch: 87 Elapsed: 0m 8s Percent of epoch Complete: (0%) 630.4015\n",
      "Epoch: 88 Elapsed: 0m 8s Percent of epoch Complete: (0%) 626.6070\n",
      "Epoch: 89 Elapsed: 0m 8s Percent of epoch Complete: (0%) 632.0329\n",
      "Epoch: 90 Elapsed: 0m 8s Percent of epoch Complete: (0%) 628.1836\n",
      "Epoch: 91 Elapsed: 0m 9s Percent of epoch Complete: (0%) 629.4947\n",
      "Epoch: 92 Elapsed: 0m 9s Percent of epoch Complete: (0%) 630.4238\n",
      "Epoch: 93 Elapsed: 0m 9s Percent of epoch Complete: (0%) 627.6014\n",
      "Epoch: 94 Elapsed: 0m 9s Percent of epoch Complete: (0%) 629.8985\n",
      "Epoch: 95 Elapsed: 0m 9s Percent of epoch Complete: (0%) 628.5240\n",
      "Epoch: 96 Elapsed: 0m 9s Percent of epoch Complete: (0%) 628.1989\n",
      "Epoch: 97 Elapsed: 0m 9s Percent of epoch Complete: (0%) 627.0578\n",
      "Epoch: 98 Elapsed: 0m 9s Percent of epoch Complete: (0%) 629.1680\n",
      "Epoch: 99 Elapsed: 0m 9s Percent of epoch Complete: (0%) 629.1399\n"
     ]
    }
   ],
   "source": [
    "dec = Decoder().to(device)\n",
    "pred = Pred().to(device)\n",
    "\n",
    "trainEpochs(dec, pred, 100, print_every=1, eval_every = 250, learning_rate = 0.003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5debfdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(enc, dec, pred):\n",
    "    test_dl, test_len = dm.get_loader(test=True)\n",
    "    trip_ids = []\n",
    "    pred_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc.eval()\n",
    "        dec.eval()\n",
    "        pred.eval()\n",
    "        for i, (trip_id, in_feats) in enumerate(test_dl):\n",
    "            in_feats = in_feats.to(device)\n",
    "            \n",
    "            hidden = enc(in_feats)\n",
    "\n",
    "            di = torch.Tensor([0]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            dh = hidden.to(device)\n",
    "                                \n",
    "            for i in range(200):\n",
    "                do, dh = dec(di, dh)\n",
    "            \n",
    "                topv, topi = do.topk(1, dim=2)\n",
    "                di = topv.detach().to(device)  # detach from history as input\n",
    "                if (topv == EOS):\n",
    "                    break\n",
    "            \n",
    "            pred_in = torch.cat((in_feats, dh.squeeze(0)), dim=1).to(device)\n",
    "            pred_time = pred(pred_in)\n",
    "            trip_ids.append(trip_id[0])\n",
    "            pred_times.append(pred_time.item())\n",
    "\n",
    "    df_sample = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "    df_sample[\"TRAVEL_TIME\"] = pred_times\n",
    "    df_sample.to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13e64d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission(enc, dec, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68606be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747bc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
