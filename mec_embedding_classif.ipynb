{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ebde34",
   "metadata": {},
   "source": [
    "**Model Config**\n",
    "Use Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03d65a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import data_module as dm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SOS = 0\n",
    "EOS = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58638407",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "- Load CSV file dataset\n",
    "- Create Torch Dataset\n",
    "- Create Torch DataLoader\n",
    "- Create padding func (with insertion of SOS and EOS tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d4005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl, train_len, test_len = dm.get_loader(use_embedding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f05e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98, 64, 1])\n",
      "torch.Size([73, 64, 1])\n",
      "tensor([[1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.2060e+03],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [4.2000e+02],\n",
      "        [8.0000e+01],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [9.8500e+02],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [2.2000e+02],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [2.0900e+02],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0030e+03],\n",
      "        [1.0900e+03],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [4.7000e+01],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [8.3700e+02],\n",
      "        [1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_dl)\n",
    "fs = next(it)\n",
    "tt = fs[1]\n",
    "print(tt.shape)\n",
    "tt = tt[:-25,:,:]\n",
    "print(tt.shape)\n",
    "print(tt[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82379ba",
   "metadata": {},
   "source": [
    "**Model Building**\n",
    "- Encoder-Decoder architecture\n",
    "    - Encoder -> MLP or CNN\n",
    "    - Decoder -> LSTM RNN\n",
    "    - Batch Normalization in both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6622884",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model Architectures \"\"\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(594, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.dropout(F.relu(self.bn1(self.fc1(input.float()))))\n",
    "        x = self.dropout(F.relu(self.bn2(self.fc2(x))))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = x.view(1, -1, 128)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "845ec1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(len(list(dm.coord_2_idx.keys())), 256)\n",
    "        self.gru = nn.GRU(256, 128)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, in_feats, hidden):\n",
    "        embedded = self.emb(in_feats.to(torch.int64)).to(device)\n",
    "        embedded = F.relu(embedded.squeeze(2))\n",
    "        do, hidden = self.gru(embedded, hidden.to(torch.float))\n",
    "        do = self.softmax(do)\n",
    "        \n",
    "        return do, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "081c2988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pred(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pred, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(128+594, 512) # Take in 128 feats from gru hidden plus categoricals\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512,1024)\n",
    "        self.bn2 = nn.BatchNorm1d(1024)\n",
    "        self.fc3 = nn.Linear(1024, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 466)\n",
    "        self.sm = nn.Softmax(1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "    def forward(self, in_feats):\n",
    "        x = self.dropout(F.relu(self.bn1(self.fc1(in_feats))))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        x = self.sm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2014e845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12350546\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder()\n",
    "dec = Decoder()\n",
    "pred = Pred()\n",
    "\n",
    "print(sum(p.numel() for p in enc.parameters() if p.requires_grad) + \n",
    "      sum(p.numel() for p in dec.parameters() if p.requires_grad) +\n",
    "      sum(p.numel() for p in pred.parameters() if p.requires_grad))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62bbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSE, self).__init__()\n",
    "        \n",
    "        self.crit = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        return torch.sqrt(self.crit(x.squeeze(0).to(torch.float64), y.squeeze(0).to(torch.float64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ede66",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "313b17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer helper functions from \n",
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#the-seq2seq-model\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s' % (asMinutes(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cafa9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(in_feats, cl, tt, encoder, decoder, pred, enc_optim, dec_optim, pred_optim, criterion):\n",
    "    enc_optim.zero_grad()\n",
    "    dec_optim.zero_grad()\n",
    "    pred_optim.zero_grad()\n",
    "    \n",
    "    in_feats = in_feats.to(device)\n",
    "    cl = cl.to(device)\n",
    "    tt = tt.to(device)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    \n",
    "    hidden = encoder(in_feats)\n",
    "    \n",
    "    \n",
    "    di = cl[0].unsqueeze(0).to(device)\n",
    "    dh = hidden.to(device)\n",
    "                    \n",
    "    use_teacher_forcing = True if random.random() < 0.3 else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for i in range(len(cl) - 1):\n",
    "            do, dh = decoder(di, dh)\n",
    "            do = do.to(device)\n",
    "            dh = dh.to(device)\n",
    "            di = cl[i + 1].unsqueeze(0).to(device)  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for i in range(len(cl)):\n",
    "            do, dh = decoder(di, dh)\n",
    "            do = do.to(device)\n",
    "            dh = dh.to(device)\n",
    "            \n",
    "            topv, topi = do.topk(1, dim=2)\n",
    "            di = topi.detach().to(device)  # detach from history as input\n",
    "\n",
    "    pred_in = torch.cat((in_feats.squeeze(0), dh.squeeze(0)), dim=1).to(device)\n",
    "\n",
    "    pred_idx = pred(pred_in)\n",
    "    loss += criterion(pred_idx, tt.to(torch.int64))\n",
    "    loss.backward()\n",
    "        \n",
    "    enc_optim.step()\n",
    "    dec_optim.step()\n",
    "    pred_optim.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfb3f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpochs(encoder, decoder, predictor, n_epochs, print_every=1000, eval_every = 5, learning_rate=0.003):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    enc_optim = optim.AdamW(encoder.parameters(), lr=learning_rate)\n",
    "    dec_optim = optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
    "    pred_optim = optim.AdamW(predictor.parameters(), lr=learning_rate)\n",
    "    #criterion = RMSE()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    es = optim.lr_scheduler.ReduceLROnPlateau(enc_optim, 'min', 0.25, 3)\n",
    "    ds = optim.lr_scheduler.ReduceLROnPlateau(dec_optim, 'min', 0.25, 3)\n",
    "    ps = optim.lr_scheduler.ReduceLROnPlateau(pred_optim, 'min', 0.25, 3)\n",
    "\n",
    "    \n",
    "    epoch_loss_max = math.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, data in enumerate(train_dl): # change back to train_dl\n",
    "            if i > 2:\n",
    "                break\n",
    "            in_feats = data[0]\n",
    "\n",
    "            if (in_feats.shape[0] != 64):\n",
    "                continue\n",
    "\n",
    "            cl = data[1]\n",
    "\n",
    "            tt = data[2]\n",
    "            loss = train(in_feats, cl, tt, encoder, decoder, pred, enc_optim, dec_optim, pred_optim, criterion)\n",
    "            print_loss_total += loss\n",
    "\n",
    "\n",
    "            if (i % 1000 == 0):\n",
    "                if (print_loss_total < epoch_loss_max):\n",
    "                    epoch_loss_max = print_loss_total\n",
    "                    torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'encoder_state_dict': encoder.state_dict(),\n",
    "                            'encoder_optimizer_state_dict': enc_optim.state_dict(),\n",
    "                            'decoder_state_dict': decoder.state_dict(),\n",
    "                            'decoder_optimizer_state_dict': dec_optim.state_dict(),\n",
    "                            'loss': print_loss_total,\n",
    "                            }, 'model.pt')\n",
    "\n",
    "            if (i % print_every == 0) and (i != 0): # Change back to i\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "\n",
    "\n",
    "\n",
    "                print('Epoch: %d Elapsed: %s Percent of epoch Complete: (%d%%) %.4f' % (epoch, timeSince(start, i / (train_len / 128)),\n",
    "                                                                  i / (train_len / 64) * 100, print_loss_avg))\n",
    "\n",
    "\n",
    "            if (i % eval_every == 0) and (i != 0):\n",
    "                print('*****EVALUATING*****')\n",
    "                eval_loss = eval_epoch(encoder, decoder, pred, epoch)\n",
    "                es.step(eval_loss)\n",
    "                ds.step(eval_loss)\n",
    "                ps.step(eval_loss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "258893e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, pred, in_feats, cl, tt, max_len=1500):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #eval_loss = RMSE()\n",
    "        eval_loss = nn.CrossEntropyLoss()\n",
    "        in_feats = in_feats.to(device)\n",
    "        cl = cl.to(device)\n",
    "        tt = tt.to(device)\n",
    "\n",
    "        hidden = encoder(in_feats)\n",
    "\n",
    "\n",
    "        di = torch.zeros((1,64,1)).to(device)\n",
    "        dh = hidden.to(device)\n",
    "\n",
    "        for i in range(1000):\n",
    "            do, dh = dec(di, dh)\n",
    "\n",
    "            topv, topi = do.topk(1, dim=2)\n",
    "            di = topi.detach().to(device)  # detach from history as input\n",
    "            \n",
    "        pred_in = torch.cat((in_feats.squeeze(0), dh.squeeze(0)), dim=1).to(device)\n",
    "        pred_time = pred(pred_in)\n",
    "        l = eval_loss(pred_time, tt.to(torch.int64))\n",
    "        \n",
    "        return l\n",
    "    \n",
    "def eval_epoch(encoder, decoder, pred, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    pred.eval()\n",
    "\n",
    "    accs = []\n",
    "    for i, data in enumerate(test_dl):\n",
    "        in_feats = data[0]\n",
    "        if (in_feats.shape[0] != 64):\n",
    "            continue\n",
    "        cl = data[1]\n",
    "        tt = data[2]\n",
    "        accs.append(evaluate(encoder, decoder, pred, in_feats, cl, tt))\n",
    "        \n",
    "        if (i > 100):\n",
    "            break\n",
    "    \n",
    "    epoch_acc = (sum(accs) / len(accs)) if len(accs) > 0 else 0\n",
    "    print('Epoch: %d, Loss on test: %.4f' % (epoch, epoch_acc))\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    pred.train()\n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b391701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Elapsed: 0m 0s Percent of epoch Complete: (0%) 12.2860\n",
      "Epoch: 0 Elapsed: 0m 0s Percent of epoch Complete: (0%) 6.1303\n",
      "Epoch: 1 Elapsed: 0m 1s Percent of epoch Complete: (0%) 12.2706\n",
      "Epoch: 1 Elapsed: 0m 1s Percent of epoch Complete: (0%) 6.1467\n",
      "Epoch: 2 Elapsed: 0m 2s Percent of epoch Complete: (0%) 12.2398\n",
      "Epoch: 2 Elapsed: 0m 2s Percent of epoch Complete: (0%) 6.1478\n",
      "Epoch: 3 Elapsed: 0m 2s Percent of epoch Complete: (0%) 12.2020\n",
      "Epoch: 3 Elapsed: 0m 3s Percent of epoch Complete: (0%) 6.0854\n",
      "Epoch: 4 Elapsed: 0m 5s Percent of epoch Complete: (0%) 12.2801\n",
      "Epoch: 4 Elapsed: 0m 5s Percent of epoch Complete: (0%) 6.1479\n",
      "Epoch: 5 Elapsed: 0m 5s Percent of epoch Complete: (0%) 12.2645\n",
      "Epoch: 5 Elapsed: 0m 6s Percent of epoch Complete: (0%) 6.1166\n",
      "Epoch: 6 Elapsed: 0m 6s Percent of epoch Complete: (0%) 12.2176\n",
      "Epoch: 6 Elapsed: 0m 7s Percent of epoch Complete: (0%) 6.1166\n",
      "Epoch: 7 Elapsed: 0m 7s Percent of epoch Complete: (0%) 12.2489\n",
      "Epoch: 7 Elapsed: 0m 7s Percent of epoch Complete: (0%) 6.1322\n",
      "Epoch: 8 Elapsed: 0m 8s Percent of epoch Complete: (0%) 12.2176\n",
      "Epoch: 8 Elapsed: 0m 8s Percent of epoch Complete: (0%) 6.1322\n",
      "Epoch: 9 Elapsed: 0m 8s Percent of epoch Complete: (0%) 12.2645\n",
      "Epoch: 9 Elapsed: 0m 8s Percent of epoch Complete: (0%) 6.1479\n",
      "Epoch: 10 Elapsed: 0m 9s Percent of epoch Complete: (0%) 12.2645\n",
      "Epoch: 10 Elapsed: 0m 9s Percent of epoch Complete: (0%) 6.1166\n",
      "Epoch: 11 Elapsed: 0m 10s Percent of epoch Complete: (0%) 12.2489\n",
      "Epoch: 11 Elapsed: 0m 10s Percent of epoch Complete: (0%) 6.1166\n",
      "Epoch: 12 Elapsed: 0m 10s Percent of epoch Complete: (0%) 12.2020\n",
      "Epoch: 12 Elapsed: 0m 10s Percent of epoch Complete: (0%) 6.1322\n",
      "Epoch: 13 Elapsed: 0m 11s Percent of epoch Complete: (0%) 12.2645\n",
      "Epoch: 13 Elapsed: 0m 11s Percent of epoch Complete: (0%) 6.1322\n",
      "Epoch: 14 Elapsed: 0m 12s Percent of epoch Complete: (0%) 12.2332\n",
      "Epoch: 14 Elapsed: 0m 12s Percent of epoch Complete: (0%) 6.1166\n",
      "Epoch: 15 Elapsed: 0m 12s Percent of epoch Complete: (0%) 12.2645\n",
      "Epoch: 15 Elapsed: 0m 12s Percent of epoch Complete: (0%) 6.1322\n",
      "Epoch: 16 Elapsed: 0m 13s Percent of epoch Complete: (0%) 12.2332\n",
      "Epoch: 16 Elapsed: 0m 13s Percent of epoch Complete: (0%) 6.1166\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14247/2901342123.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainEpochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.003\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_14247/3957137278.py\u001b[0m in \u001b[0;36mtrainEpochs\u001b[0;34m(encoder, decoder, predictor, n_epochs, print_every, eval_every, learning_rate)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14247/2201140039.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(in_feats, cl, tt, encoder, decoder, pred, enc_optim, dec_optim, pred_optim, criterion)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mpred_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0menc_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "enc = Encoder().to(device)\n",
    "dec = Decoder().to(device)\n",
    "pred = Pred().to(device)\n",
    "\n",
    "trainEpochs(enc, dec, pred, 100, print_every=1, eval_every = 500, learning_rate = 0.003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5debfdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(enc, dec, pred):\n",
    "    test_dl, test_len = dm.get_loader(test=True)\n",
    "    trip_ids = []\n",
    "    pred_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc.eval()\n",
    "        dec.eval()\n",
    "        pred.eval()\n",
    "        for i, (trip_id, in_feats) in enumerate(test_dl):\n",
    "            in_feats = in_feats.to(device)\n",
    "            \n",
    "            hidden = enc(in_feats)\n",
    "\n",
    "            di = torch.Tensor([0]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            dh = hidden.to(device)\n",
    "                                \n",
    "            for i in range(150):\n",
    "                do, dh = dec(di, dh)\n",
    "            \n",
    "                topv, topi = do.topk(1, dim=2)\n",
    "                di = topi.detach().to(device)  # detach from history as input\n",
    "                if (topi == i):\n",
    "                    break\n",
    "                    \n",
    "                if (i > 145):\n",
    "                    print('reached 150')\n",
    "            \n",
    "            pred_in = torch.cat((in_feats, dh.squeeze(0)), dim=1).to(device)\n",
    "            pred_idx = pred(pred_in)\n",
    "            \n",
    "            topv, topi = pred_idx.topk(1, dim=1)\n",
    "            pred_time = dm.i2t[topi.item()]\n",
    "            \n",
    "            trip_ids.append(trip_id[0])\n",
    "            pred_times.append(pred_time)\n",
    "\n",
    "    df_sample = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "    df_sample[\"TRAVEL_TIME\"] = pred_times\n",
    "    df_sample.to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e64d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n",
      "reached 150\n"
     ]
    }
   ],
   "source": [
    "submission(enc, dec, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68606be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747bc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
